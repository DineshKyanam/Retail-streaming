# ğŸ›’ Real-Time Retail Data Streaming Platform

A production-grade real-time retail data pipeline built using **Apache Kafka, Spark Structured Streaming, Delta Lake, Snowflake, and Airflow**.  
This project simulates a retail company's end-to-end data flow â€” from live transaction ingestion to data warehousing and analytics â€” demonstrating a complete modern cloud-native data engineering pipeline.

---

## ğŸ§± Architecture (Modern Data Lakehouse)

**Flow:** Mock Producer â†’ Kafka â†’ Spark Structured Streaming â†’ Delta Lake (S3) â†’ Snowflake â†’ dbt + Airflow â†’ BI Dashboard  

### ğŸ—ºï¸ Architecture Diagram
![Architecture Diagram](docs/architecture.png)

Retail POS / E-Commerce Data
â”‚
â–¼
Apache Kafka (Topics)
â”‚
â–¼
Spark Structured Streaming
(Transform + Aggregate)
â”‚
â–¼
Delta Lake (Bronze â†’ Silver â†’ Gold)
â”‚
â–¼
Snowflake (Analytics Layer)
â”‚
â–¼
Power BI / Tableau / Streamlit

yaml
Copy code

---

## âš™ï¸ Getting Started

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/DineshKyanam/Retail-streaming.git
cd Retail-streaming
2ï¸âƒ£ Start the Local Environment
Make sure you have Docker Desktop running, then start your local stack:

bash
Copy code
docker compose up -d
This starts Kafka, Spark, and Airflow services locally.

3ï¸âƒ£ Generate Streaming Data
Run the mock data producer to continuously push transactions into Kafka:

bash
Copy code
python producer/produce_transactions.py
4ï¸âƒ£ Process Data with Spark
Run the real-time Spark streaming job:

bash
Copy code
spark-submit spark_streaming/stream_processor.py
5ï¸âƒ£ Monitor via Airflow
Access the Airflow UI at http://localhost:8080.
Enable and trigger the DAG retail_streaming_dag to orchestrate the ETL flow.

ğŸ“Š Sample Outputs
Below is a snapshot of daily retail KPIs produced by the streaming pipeline:

date	orders	revenue_usd	avg_order_value_usd	streaming_latency_ms
2025-10-01	1023	34456.78	33.69	320
2025-10-02	1289	45123.56	35.00	295
2025-10-03	1411	49320.47	34.95	270

ğŸ“ Full Output: outputs/aggregates_daily.csv

ğŸ“ˆ KPI Dashboards
ğŸ›ï¸ Orders Over Time

ğŸ’° Revenue Over Time

ğŸ’¼ Business Value
Enables real-time sales and revenue analytics

Delivers sub-second latency metrics for faster decisions

Supports fraud detection, dynamic pricing, and inventory optimization

Combines streaming + batch processing in a unified Lakehouse architecture

Demonstrates production-grade orchestration using Airflow and dbt

ğŸš€ Future Enhancements
Integrate Kafka Connect and Schema Registry

Add Grafana dashboards for live monitoring

Deploy to AWS MSK + EMR + ECS for cloud scalability

Implement CI/CD pipelines for automated deployments

Add unit tests and alerting for production reliability

ğŸ‘¤ Author
Dinesh Kyanam
ğŸ“ Overland Park, KS
ğŸ“§ kyanamdinesh18@gmail.com
ğŸ”— linkedin.com/in/dinesh-kyanam-180b611a2
ğŸ’» github.com/DineshKyanam




