# ğŸ›’ Real-Time Retail Data Streaming Platform

A production-grade **real-time retail data pipeline** built using **Apache Kafka**, **Spark Structured Streaming**, **Delta Lake**, **Snowflake**, and **Airflow**.  
This project simulates a retail companyâ€™s end-to-end data flow â€” from **live transaction ingestion** to **data warehousing and analytics** â€” demonstrating your ability to build **modern, cloud-native data pipelines**.

---

## ğŸ§­ Architecture (Modern Data Lakehouse)

![Architecture](docs/architecture.png)

**Flow:**  
`Mock Producer` â†’ `Kafka` â†’ `Spark Structured Streaming` â†’ `Delta Lake (S3)` â†’ `Snowflake` â†’ `dbt + Airflow` â†’ `BI Dashboard`

---

## ğŸ§  Project Overview

This system emulates a **real-world retail data environment** where thousands of transactions (POS + online) are generated every minute.  
The data flows through a **real-time streaming pipeline** that transforms, aggregates, and loads results into analytical tables for dashboards and insights.

**Key Goals:**
- Real-time ingestion using Kafka  
- Streaming transformation with PySpark  
- Delta Lake storage for reliability and time-travel  
- Automated orchestration with Airflow  
- Snowflake for downstream analytics  
- End-to-end observability via dashboards  

---

## ğŸ§± Tech Stack

| Category | Tools / Technologies |
|-----------|----------------------|
| **Languages** | Python, PySpark, SQL |
| **Streaming** | Apache Kafka, Spark Structured Streaming |
| **Storage / Lakehouse** | Delta Lake on AWS S3, Snowflake |
| **Orchestration** | Apache Airflow, dbt |
| **Infrastructure** | Docker, AWS |
| **Visualization** | Power BI / Tableau / Streamlit (optional) |
| **Version Control** | Git & GitHub |

---

## ğŸ—‚ï¸ Project Structure

Retail-streaming/
â”‚
â”œâ”€â”€ producer/ â†’ Generates mock retail transactions
â”‚ â””â”€â”€ produce_transactions.py
â”‚
â”œâ”€â”€ spark_streaming/ â†’ Spark job to process data in real-time
â”‚ â””â”€â”€ stream_processor.py
â”‚
â”œâ”€â”€ loaders/ â†’ SQL scripts for Snowflake upsert logic
â”‚ â””â”€â”€ snowflake_upsert.sql
â”‚
â”œâ”€â”€ airflow/dags/ â†’ Airflow DAG for orchestrating the pipeline
â”‚ â””â”€â”€ retail_streaming_dag.py
â”‚
â”œâ”€â”€ mock/ â†’ Scripts to simulate batch KPIs
â”‚ â”œâ”€â”€ run_mock_stream.py
â”‚ â””â”€â”€ run_batch_kpis.py
â”‚
â”œâ”€â”€ outputs/ â†’ Processed KPI and analytics results
â”‚ â””â”€â”€ aggregates_daily.csv
â”‚
â”œâ”€â”€ docs/ â†’ Documentation and diagrams
â”‚ â””â”€â”€ architecture.png
â”‚
â””â”€â”€ docker-compose.yml â†’ Local environment setup for Kafka/Spark

yaml
Copy code

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/DineshKyanam/Retail-streaming.git
cd Retail-streaming
2ï¸âƒ£ Start the Local Environment
Make sure you have Docker Desktop running, then start your local stack:

bash
Copy code
docker compose up -d
This starts Kafka, Spark, and Airflow services locally.

3ï¸âƒ£ Generate Streaming Data
Run the mock data producer to continuously push transactions into Kafka:

bash
Copy code
python producer/produce_transactions.py
4ï¸âƒ£ Process Data with Spark
Run the real-time Spark streaming job:

bash
Copy code
spark-submit spark_streaming/stream_processor.py
5ï¸âƒ£ Monitor via Airflow
Access Airflow UI (default: http://localhost:8080)
Enable and trigger the DAG retail_streaming_dag to orchestrate the ETL flow.

ğŸ“Š Sample Outputs
Below is a snapshot of daily retail KPIs produced by the streaming pipeline:

date	orders	revenue_usd	avg_order_value_usd	streaming_latency_ms
2025-10-01	1023	34456.78	33.69	320
2025-10-02	1289	45123.56	35.00	295
2025-10-03	1411	49320.47	34.95	270

ğŸ“ Download full output â†’ outputs/aggregates_daily.csv

ğŸ“ˆ Business Value
Enables real-time sales and revenue analytics

Delivers sub-second latency metrics for decision-making

Supports fraud detection, dynamic pricing, and inventory optimization

Combines streaming + batch pipelines in a unified Lakehouse architecture

ğŸ§© Future Enhancements
Integrate Kafka Connect and Schema Registry

Add Grafana dashboards for live metrics

Deploy to AWS MSK + EMR + ECS for cloud scalability

Add CI/CD pipelines for production-grade workflows

ğŸ‘¤ Author
Dinesh Kyanam
Data Engineer | AWS | PySpark | Snowflake | Kafka | Airflow
ğŸ“§ dineshkyanam@gmail.com
ğŸ”— LinkedIn
